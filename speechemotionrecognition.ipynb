{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T21:09:52.145822Z","iopub.status.busy":"2023-11-23T21:09:52.145490Z","iopub.status.idle":"2023-11-23T21:09:56.103604Z","shell.execute_reply":"2023-11-23T21:09:56.102703Z","shell.execute_reply.started":"2023-11-23T21:09:52.145794Z"},"trusted":true},"outputs":[],"source":["import wandb\n","wandb.login(key=\"APIKEY\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T21:09:56.108715Z","iopub.status.busy":"2023-11-23T21:09:56.108470Z","iopub.status.idle":"2023-11-23T21:10:19.692281Z","shell.execute_reply":"2023-11-23T21:10:19.691266Z","shell.execute_reply.started":"2023-11-23T21:09:56.108694Z"},"trusted":true},"outputs":[],"source":["!pip install transformers datasets evaluate accelerate librosa\n","!pip install --upgrade gdown"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T21:10:19.693937Z","iopub.status.busy":"2023-11-23T21:10:19.693636Z","iopub.status.idle":"2023-11-23T21:10:42.846983Z","shell.execute_reply":"2023-11-23T21:10:42.845788Z","shell.execute_reply.started":"2023-11-23T21:10:19.693912Z"},"trusted":true},"outputs":[],"source":["!pip install datasets==2.14.6\n","!pip install pandas==1.5.3"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-11-23T21:10:42.850295Z","iopub.status.busy":"2023-11-23T21:10:42.849955Z","iopub.status.idle":"2023-11-23T21:10:52.824489Z","shell.execute_reply":"2023-11-23T21:10:52.823697Z","shell.execute_reply.started":"2023-11-23T21:10:42.850243Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","import os\n","from glob import glob\n","\n","# from tqdm import tqdm\n","from tqdm.notebook import tqdm\n","import librosa\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import (\n","    confusion_matrix,\n","    classification_report,\n","    recall_score,\n","    precision_score,\n","    accuracy_score,\n","    ConfusionMatrixDisplay,\n","    f1_score\n",")\n","from scipy.stats import spearmanr\n","import torch\n","from datasets import load_dataset, load_metric\n","from transformers import (\n","    AutoFeatureExtractor,\n","    AutoModelForAudioClassification,\n","    TrainingArguments,\n","    Trainer,\n","    AdamW,\n","    EarlyStoppingCallback\n",")\n","import matplotlib.pyplot as plt\n","\n","SEED=3\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","# import os\n","# for dirname, _, filenames in os.walk('/kaggle/input/crema-d/CREMA-D-master/AudioMP3'):\n","#     for filename in filenames:\n","#         print(filename)\n","save_path = \"/kaggle/working\"\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","metadata":{},"source":["## Prepare Data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["data = []\n","\n","for path in tqdm(glob(\"/kaggle/input/d/return0root/crema-d/CREMA-D/AudioWAV/*.wav\")):\n","    name = str(path).split('/')[-1].split('.')[0]\n","    actor_id, sentence, emotion, level = name.split('_')\n","    try:\n","        y,sr = librosa.load(path, sr=16000)\n","        data.append({\n","            \"file\": path,\n","            \"actor_id\": actor_id,\n","            \"sentence\": sentence,\n","            \"label\": emotion,\n","            \"level\": level\n","        })\n","    except Exception as e:\n","        raise(e)\n","df = pd.DataFrame(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df = pd.DataFrame(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# SentenceFilenames.csv - list of movie files used in study\n","# finishedEmoResponses.csv - the first emotional response with timing.\n","# finishedResponses.csv - the final emotional Responses with emotion levels with repeated and practice responses removed, used to tabulate the votes\n","\n","df_sentence = pd.read_csv('/kaggle/input/d/return0root/crema-d/CREMA-D/SentenceFilenames.csv')\n","df_first_resp = pd.read_csv('/kaggle/input/d/return0root/crema-d/CREMA-D/finishedEmoResponses.csv')\n","df_final_resp = pd.read_csv('/kaggle/input/d/return0root/crema-d/CREMA-D/finishedResponses.csv', low_memory=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_first_resp['numTries'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_final_resp['numTries'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_df, dev_df = train_test_split(df, test_size=0.3, random_state=SEED,\n","                                    stratify=df[\"label\"])\n","dev_df, test_df = train_test_split(dev_df, test_size=0.5, random_state=SEED,\n","                                   stratify=dev_df[\"label\"])\n","\n","train_df = train_df.reset_index(drop=True)\n","dev_df = dev_df.reset_index(drop=True)\n","test_df = test_df.reset_index(drop=True)\n","\n","# remove unused features in training models\n","# train_df.drop(['actor_id','sentence', 'level'], axis=1, inplace=True)\n","# dev_df.drop(['actor_id','sentence', 'level'], axis=1, inplace=True)\n","# test_df.drop(['actor_id','sentence', 'level'], axis=1, inplace=True)\n","\n","train_df.to_csv(f\"{save_path}/train.csv\", encoding=\"utf-8\", index=False)\n","dev_df.to_csv(f\"{save_path}/dev.csv\", encoding=\"utf-8\", index=False)\n","test_df.to_csv(f\"{save_path}/test.csv\", encoding=\"utf-8\", index=False)\n","\n","print(train_df.shape)\n","print(dev_df.shape)\n","print(test_df.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["data_files = {\n","    \"train\": f\"{save_path}/train.csv\",\n","    \"validation\": f\"{save_path}/dev.csv\",\n","    \"test\": f\"{save_path}/test.csv\"\n","}\n","\n","dataset = load_dataset(\"csv\", data_files=data_files)\n","train_dataset = dataset[\"train\"]\n","dev_dataset = dataset[\"validation\"]\n","test_dataset = dataset[\"test\"]\n","\n","\n","print(dataset)\n","\n","label_list = sorted(train_dataset.unique('label'))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Base = 90M parameters; Large = 300M parameters\n","\n","model_name_or_path = \"jonatasgrosman/wav2vec2-large-xlsr-53-english\" # pre-trained on multi-lingual speech, fine-tuning on English\n","\n","# Feel free to look for and experiment with other models at HuggingFace Hub https://huggingface.co/"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["feature_extractor=AutoFeatureExtractor.from_pretrained(model_name_or_path)\n","model=AutoModelForAudioClassification.from_pretrained(model_name_or_path,\n","                                      num_labels=len(train_dataset.unique(\"label\")),\n","                                      label2id={label: i for i, label in enumerate(label_list)},\n","                                      id2label={i: label for i, label in enumerate(label_list)}\n","                                      )\n","model.freeze_feature_encoder()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def label_to_id(label, label_list):\n","    if len(label_list) > 0:\n","        return label_list.index(label) if label in label_list else -1\n","    return label\n","def prepare_example(example):\n","    example[\"audio\"], example[\"sampling_rate\"] = librosa.load(example[\"file\"], sr=feature_extractor.sampling_rate)\n","    example[\"duration_in_seconds\"] = len(example[\"audio\"]) / feature_extractor.sampling_rate\n","    example[\"label\"] = label_to_id(example[\"label\"], label_list)\n","    return example\n","def preprocess_function(examples):\n","    audio_arrays = examples[\"audio\"]\n","    inputs = feature_extractor(\n","        audio_arrays,\n","        sampling_rate=feature_extractor.sampling_rate\n","    )\n","    return inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["dataset = dataset.map(prepare_example, remove_columns=['file'])\n","dataset = dataset.map(preprocess_function, batched=True, batch_size=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# delete processed data\n","# !rm -rf /kaggle/working/data/preprocessed"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["dataset.save_to_disk(f\"{save_path}/data/preprocessed/\")"]},{"cell_type":"markdown","metadata":{},"source":["## Train"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T21:10:52.826175Z","iopub.status.busy":"2023-11-23T21:10:52.825592Z","iopub.status.idle":"2023-11-23T21:10:52.954875Z","shell.execute_reply":"2023-11-23T21:10:52.953907Z","shell.execute_reply.started":"2023-11-23T21:10:52.826147Z"},"trusted":true},"outputs":[],"source":["from datasets import load_from_disk\n","\n","dataset = load_from_disk(f\"{save_path}/data/preprocessed/\")\n","train_dataset = dataset[\"train\"]\n","dev_dataset = dataset[\"validation\"]\n","test_dataset = dataset[\"test\"]\n","\n","\n","print(dataset)\n","\n","label_list = sorted(train_dataset.unique('label'))\n","label_list"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T21:10:52.956474Z","iopub.status.busy":"2023-11-23T21:10:52.956132Z","iopub.status.idle":"2023-11-23T21:10:52.962408Z","shell.execute_reply":"2023-11-23T21:10:52.961494Z","shell.execute_reply.started":"2023-11-23T21:10:52.956441Z"},"trusted":true},"outputs":[],"source":["print(torch.cuda.memory_allocated())\n","print(torch.cuda.memory_reserved())\n","torch.cuda.empty_cache()\n","print(torch.cuda.memory_allocated())\n","print(torch.cuda.memory_reserved())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T21:11:35.668287Z","iopub.status.busy":"2023-11-23T21:11:35.667587Z","iopub.status.idle":"2023-11-23T22:42:25.924316Z","shell.execute_reply":"2023-11-23T22:42:25.922426Z","shell.execute_reply.started":"2023-11-23T21:11:35.668252Z"},"trusted":true},"outputs":[],"source":["# Batch size = per_device_train_batch_size * gradient_accumulation_steps\n","# Parameters to tune: learning rate, epochs, (batch size)\n","# More details on hyperparameter tuning in https://github.com/google-research/tuning_playbook\n","\n","def compute_metrics(pred):\n","    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n","    predictions = np.argmax(pred.predictions, axis=1)\n","    accuracy = accuracy_score(pred.label_ids, predictions)\n","    precision = precision_score(pred.label_ids, predictions, average='macro')\n","    recall = recall_score(pred.label_ids, predictions, average='macro')\n","    f1 = f1_score(pred.label_ids, predictions, average='macro')\n","    return {\"accuracy\": accuracy,\n","            \"precision\": precision,\n","            \"recall\": recall,\n","            \"f1\": f1}\n","\n","\n","learning_rates = [1e-3, 1e-4, 1e-5] # first round\n","num_epochs = 5\n","# learning_rates = [1.5e-4, 1e-4, 0.5e-4] # second round\n","# num_epochs = 10\n","# learning_rates = [1.25e-4, 1.5e-4, 1.75e-4] # third round\n","# num_epochs = 15\n","evaluations = []\n","\n","model_name_or_path = \"jonatasgrosman/wav2vec2-large-xlsr-53-english\" # pre-trained on multi-lingual speech, fine-tuning on English\n","\n","feature_extractor=AutoFeatureExtractor.from_pretrained(model_name_or_path)\n","\n","for lr in learning_rates:\n","    torch.cuda.empty_cache()\n","  # 🐝 1️⃣ Start a new run to track this script\n","    with wandb.init(\n","        # Set the project where this run will be logged\n","        project=\"SER\",\n","        entity=\"black-noodles\",\n","        # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n","        name=f\"{model_name_or_path}_{lr}_{num_epochs}_cosine\", \n","        # Track hyperparameters and run metadata\n","        config={\n","        \"learning_rate\": lr,\n","        \"architecture\": model_name_or_path,\n","        \"dataset\": \"CREMA-D\",\n","        \"epochs\": num_epochs,\n","    }):\n","        # renew model\n","        model=AutoModelForAudioClassification.from_pretrained(model_name_or_path,\n","                                              num_labels=len(train_dataset.unique(\"label\")),\n","                                              label2id={label: i for i, label in enumerate(label_list)},\n","                                              id2label={i: label for i, label in enumerate(label_list)}\n","                                              )\n","        model.freeze_feature_encoder()\n","        \n","        # start training\n","        training_args = TrainingArguments(\n","            output_dir=f\"{save_path}/{model_name_or_path}-speech-emotion-recognition\",\n","            per_device_train_batch_size=32, # require more GPU memory, this set can exploit 16GB memory\n","            gradient_accumulation_steps=4,\n","            per_device_eval_batch_size=32,\n","            num_train_epochs=num_epochs,\n","            warmup_ratio=0.1,\n","            learning_rate=lr,\n","            evaluation_strategy = \"epoch\",\n","            save_strategy = \"epoch\",\n","            save_total_limit=2,\n","            logging_steps=10,\n","            load_best_model_at_end=True,\n","            metric_for_best_model='f1',\n","            greater_is_better=True,\n","            push_to_hub=False,\n","            gradient_checkpointing=True,\n","            fp16=True,\n","            report_to=None,\n","            lr_scheduler_type=\"cosine\"\n","        )\n","\n","        # optimizer = AdamW(model.parameters(), lr = lr) \n","        # lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=5e-6, verbose=True)\n","\n","\n","        trainer = Trainer(\n","            model=model,\n","            args=training_args,\n","            compute_metrics=compute_metrics,\n","            train_dataset=train_dataset,\n","            eval_dataset=dev_dataset,\n","            tokenizer=feature_extractor,\n","            # optimizers= (optimizer, lr_scheduler),\n","            callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n","        )\n","\n","\n","        trainer.train()\n","\n","        predictions = trainer.predict(dev_dataset)\n","        \n","        result = compute_metrics(predictions)\n","\n","        wandb.log(result)\n","        evaluations.append(result['f1'])\n","      \n","  # Mark the run as finished\n","wandb.finish()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["best_lr = learning_rates[np.argmax(evaluations)]\n","\n","learning_rates = [best_lr*1.5, best_lr, best_lr*0.5] # second round\n","num_epochs = 10\n","evaluations = []\n","\n","model_name_or_path = \"jonatasgrosman/wav2vec2-large-xlsr-53-english\" # pre-trained on multi-lingual speech, fine-tuning on English\n","\n","feature_extractor=AutoFeatureExtractor.from_pretrained(model_name_or_path)\n","\n","for lr in learning_rates:\n","    torch.cuda.empty_cache()\n","  # 🐝 1️⃣ Start a new run to track this script\n","    with wandb.init(\n","        # Set the project where this run will be logged\n","        project=\"SER\",\n","        entity=\"black-noodles\",\n","        # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n","        name=f\"{model_name_or_path}_{lr}_{num_epochs}_cosine\", \n","        # Track hyperparameters and run metadata\n","        config={\n","        \"learning_rate\": lr,\n","        \"architecture\": model_name_or_path,\n","        \"dataset\": \"CREMA-D\",\n","        \"epochs\": num_epochs,\n","    }):\n","        # renew model\n","        model=AutoModelForAudioClassification.from_pretrained(model_name_or_path,\n","                                              num_labels=len(train_dataset.unique(\"label\")),\n","                                              label2id={label: i for i, label in enumerate(label_list)},\n","                                              id2label={i: label for i, label in enumerate(label_list)}\n","                                              )\n","        model.freeze_feature_encoder()\n","        \n","        # start training\n","        training_args = TrainingArguments(\n","            output_dir=f\"{save_path}/{model_name_or_path}-speech-emotion-recognition\",\n","            per_device_train_batch_size=32, # require more GPU memory, this set can exploit 16GB memory\n","            gradient_accumulation_steps=4,\n","            per_device_eval_batch_size=32,\n","            num_train_epochs=num_epochs,\n","            warmup_ratio=0.1,\n","            learning_rate=lr,\n","            evaluation_strategy = \"epoch\",\n","            save_strategy = \"epoch\",\n","            save_total_limit=2,\n","            logging_steps=10,\n","            load_best_model_at_end=True,\n","            metric_for_best_model='f1',\n","            greater_is_better=True,\n","            push_to_hub=False,\n","            gradient_checkpointing=True,\n","            fp16=True,\n","            report_to=None,\n","            lr_scheduler_type=\"cosine\"\n","        )\n","\n","        # optimizer = AdamW(model.parameters(), lr = lr) \n","        # lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=5e-6, verbose=True)\n","\n","\n","        trainer = Trainer(\n","            model=model,\n","            args=training_args,\n","            compute_metrics=compute_metrics,\n","            train_dataset=train_dataset,\n","            eval_dataset=dev_dataset,\n","            tokenizer=feature_extractor,\n","            # optimizers= (optimizer, lr_scheduler),\n","            callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n","        )\n","\n","\n","        trainer.train()\n","\n","        predictions = trainer.predict(dev_dataset)\n","        \n","        result = compute_metrics(predictions)\n","\n","        wandb.log(result)\n","        evaluations.append(result['f1'])\n","      \n","  # Mark the run as finished\n","wandb.finish()"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":3979186,"sourceId":6930154,"sourceType":"datasetVersion"}],"dockerImageVersionId":30559,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
